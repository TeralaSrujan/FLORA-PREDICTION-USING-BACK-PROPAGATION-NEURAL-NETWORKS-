{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SoftComputing.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "VROQumJVhlYD"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7iwlbGkhsFI"
      },
      "source": [
        "#Loading DATA\n",
        "df = pd.read_csv('iris_data.csv')\n",
        "data = np.array(df)\n",
        "x_train = np.concatenate((data[:40,:4],data[60:100,:4]),axis=0)\n",
        "y_train = np.concatenate((data[:40,-1],data[60:100,-1]),axis=0)\n",
        "N = len(y_train)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xvtzDLAUlX8a",
        "outputId": "421590e2-6603-413a-ca05-3fed3b2c0ded"
      },
      "source": [
        "#one hot encoding\n",
        "\n",
        "def one_hot(y):\n",
        "\tfor i in range(len(y)):\n",
        "\t\tif y[i] == 'Iris-setosa':\n",
        "\t\t\ty[i] = 0.0\n",
        "\t\telse:\n",
        "\t\t\ty[i ]= 1.0\n",
        "\treturn y\n",
        "y_train = one_hot(y_train).reshape((N,1))\n",
        "print(y_train)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.0]\n",
            " [0.0]\n",
            " [0.0]\n",
            " [0.0]\n",
            " [0.0]\n",
            " [0.0]\n",
            " [0.0]\n",
            " [0.0]\n",
            " [0.0]\n",
            " [0.0]\n",
            " [0.0]\n",
            " [0.0]\n",
            " [0.0]\n",
            " [0.0]\n",
            " [0.0]\n",
            " [0.0]\n",
            " [0.0]\n",
            " [0.0]\n",
            " [0.0]\n",
            " [0.0]\n",
            " [0.0]\n",
            " [0.0]\n",
            " [0.0]\n",
            " [0.0]\n",
            " [0.0]\n",
            " [0.0]\n",
            " [0.0]\n",
            " [0.0]\n",
            " [0.0]\n",
            " [0.0]\n",
            " [0.0]\n",
            " [0.0]\n",
            " [0.0]\n",
            " [0.0]\n",
            " [0.0]\n",
            " [0.0]\n",
            " [0.0]\n",
            " [0.0]\n",
            " [0.0]\n",
            " [0.0]\n",
            " [1.0]\n",
            " [1.0]\n",
            " [1.0]\n",
            " [1.0]\n",
            " [1.0]\n",
            " [1.0]\n",
            " [1.0]\n",
            " [1.0]\n",
            " [1.0]\n",
            " [1.0]\n",
            " [1.0]\n",
            " [1.0]\n",
            " [1.0]\n",
            " [1.0]\n",
            " [1.0]\n",
            " [1.0]\n",
            " [1.0]\n",
            " [1.0]\n",
            " [1.0]\n",
            " [1.0]\n",
            " [1.0]\n",
            " [1.0]\n",
            " [1.0]\n",
            " [1.0]\n",
            " [1.0]\n",
            " [1.0]\n",
            " [1.0]\n",
            " [1.0]\n",
            " [1.0]\n",
            " [1.0]\n",
            " [1.0]\n",
            " [1.0]\n",
            " [1.0]\n",
            " [1.0]\n",
            " [1.0]\n",
            " [1.0]\n",
            " [1.0]\n",
            " [1.0]\n",
            " [1.0]\n",
            " [1.0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TbEKh4-6le2f"
      },
      "source": [
        "# Set data type.\n",
        "# You can '#' it if you want for its insignificance.\n",
        "y_train = y_train.astype('float')\n",
        "x_train = x_train.astype('float')"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xvp555FVmAvr"
      },
      "source": [
        "# Set Hyperparameters\n",
        "# Definition :\n",
        "# \tnodes: number of neuron \n",
        "#\tsteps: number of iteration\n",
        "#\talpha: learning rate\n",
        "#\t    N: number of samples in the dataset\n",
        "np.random.seed(2)\n",
        "nodes = 6\n",
        "steps = 200\n",
        "alpha = 0.2\n",
        "\n",
        "\n",
        "# Record loss and accuracy of every training step.\n",
        "loss_data = np.zeros((steps,1))\n",
        "accuracy_data = np.zeros((steps,1))"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vemj2XQMmDoM"
      },
      "source": [
        "# Define the sigmoid as active function\n",
        "# SigmoidDerivative() is the sigmoid function's derivative.\n",
        "def Sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x.astype('float')))\n",
        "def SigmoidDerivative(x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "# Initialize weights\n",
        "# Definition :\n",
        "# \t'hws' is short for hidden layer weights.\n",
        "# \t the same with 'hws', 'ows' is short for output layer weights.\n",
        "hws = 2*np.random.random((x_train.shape[1] + 1, nodes)) - 1\n",
        "ows = 2*np.random.random((nodes + 1, y_train.shape[1])) - 1"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xyqO4wK8mHoz"
      },
      "source": [
        "# Forward Propagation\n",
        "# Definition :\n",
        "# \t'out_IL' is short for out of input layer.\n",
        "# \t'out_HL' is short for out of hidden layer.\n",
        "# \t'out_OL' is short for out of output layer.\n",
        "def ForwardPropagation(x,w1,w2,train=True):\n",
        "\n",
        "\t# adding a column of ones for bias as the output of input layer.\n",
        "\tout_IL = np.hstack((np.ones((x.shape[0],1)),x))\n",
        "\n",
        "\t# This step calculate z = b*1 + w1*x1 + w2*x2 + w3*x3 + w4*x4 \n",
        "\tout_HL =  np.dot(out_IL, w1)\n",
        "\t# feed z into active function and get out_HL as the output of hidden layer.\n",
        "\tout_HL = Sigmoid(out_HL)\n",
        "\n",
        "\t# Now pass out_HL as input data to output layer.\n",
        "\t# I think you should know what would be done by the next codes.\n",
        "\tout_HL = np.hstack((np.ones((x.shape[0], 1)),out_HL))\n",
        "\tout_OL = np.dot(out_HL, w2)\n",
        "\n",
        "\tif train:\n",
        "\t\treturn out_IL,out_HL,out_OL\n",
        "\telse:\n",
        "\t\treturn out_OL"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0J5OpLSFmP_8"
      },
      "source": [
        "# Backward Propagation\n",
        "# Definition :\n",
        "# \t'oe' is short for output layer error.\n",
        "# \t'he' is short for hidden layer error.\n",
        "# \t'd_hws' is short for derivative of hidden layer weights.\n",
        "# \t'd_ows' is short for derivative of output layer weights.\n",
        "# Here, backward propagation is not as straightforward as forward propagation.\n",
        "# So I explained it in README.md in detail.\n",
        "def BackwardPropagation(y_hat,y):\n",
        "\toe = y_hat - y\n",
        "\the = SigmoidDerivative(out_HL[:, 1:]) * np.dot(oe, ows.T[:, 1:])\n",
        "\n",
        "\td_HL = out_IL[:, :, np.newaxis] * he[: , np.newaxis, :]\n",
        "\td_hws = np.average(d_HL,axis=0)\n",
        "\n",
        "\td_OL = out_HL[:, :, np.newaxis] * oe[:, np.newaxis, :]\n",
        "\td_ows = np.average(d_OL,axis=0)\n",
        "\n",
        "\treturn d_hws,d_ows"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "W_rrCkx7SBeg",
        "outputId": "1782f7b8-e01d-4eb3-a272-1b17bfdc066f"
      },
      "source": [
        "# Cost\n",
        "# I think there is nothing need to illustrate.\n",
        "def Cost(y_hat,y,train=True):\n",
        "\tloss = np.sum(np.square(y_hat - y))\n",
        "\tprint('Loss:{}'.format(loss))\n",
        "\tloss_data[i] = loss\n",
        "\n",
        "\ty_hat = (y_hat>=0.5)\n",
        "\ty = (y == 1.0)\n",
        "\taccuracy = np.sum((y_hat == y))\n",
        "\tprint('Accuracy:{}%'.format((accuracy/N)*100))\n",
        "\taccuracy_data[i] = accuracy\n",
        "\n",
        "\tif train:\n",
        "\t\treturn loss_data,accuracy_data\n",
        "\telse:\n",
        "\t\tpass\n",
        "\n",
        "\n",
        "  # Training LOOP\n",
        "for i in range(steps):\n",
        "\tprint('------------Iterative',str(i),'------')\n",
        "\n",
        "\t# Forward Propagation\n",
        "\tout_IL,out_HL,out_OL = ForwardPropagation(x_train,hws,ows)\n",
        "\n",
        "\t# Compute loss and accuracy\n",
        "\tloss_data,accuracy_data = Cost(out_OL,y_train)\n",
        "\n",
        "\t# Backward Propagation\n",
        "\td_hws,d_ows = BackwardPropagation(out_OL,y_train)\n",
        "\n",
        "\t# Update weights\n",
        "\thws += -alpha * d_hws\n",
        "\tows += -alpha * d_ows\n",
        "\n",
        "\tprint('------------Iterative Done------','\\n')\n",
        "\n",
        "print('------------ Training Complete ------','\\n')\n",
        "\n",
        "#VISUALIZATION\n",
        "def painter(loss,accuracy,scales):\n",
        "\tfig = plt.figure()\n",
        "\tx = np.linspace(0, scales, scales)\n",
        "\tplt.plot(x, loss, label='Loss')\n",
        "\tplt.plot(x, accuracy_data, label='Accuracy')\n",
        "\tplt.xlabel('Iterative Steps')\n",
        "\tplt.ylabel('Loss')\n",
        "\tplt.title('Loss reduce following Iteration')\n",
        "\tplt.savefig('Result_tain.png')\n",
        "\t#plt.show()\n",
        "painter(loss_data,accuracy_data,steps)\n",
        "\n",
        "\n",
        "#TEST MODEL\n",
        "x_test = data[40:60,:4]\n",
        "y_test = data[40:60,-1]\n",
        "N = len(y_test)\n",
        "\n",
        "y_test = one_hot(y_test).reshape((N,1))\n",
        "\n",
        "out_OL = ForwardPropagation(x_test,hws,ows,train=False)\n",
        "print('------------ TEST Result ------')\n",
        "Cost(out_OL,y_test,train=False)\n",
        "print('------------ TEST Result ------')"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "------------Iterative 0 ------\n",
            "Loss:62.36119458494137\n",
            "Accuracy:50.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 1 ------\n",
            "Loss:41.7995364842495\n",
            "Accuracy:50.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 2 ------\n",
            "Loss:34.37035570860638\n",
            "Accuracy:50.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 3 ------\n",
            "Loss:31.118088059067333\n",
            "Accuracy:50.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 4 ------\n",
            "Loss:29.287971870189523\n",
            "Accuracy:47.5%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 5 ------\n",
            "Loss:28.030255550744975\n",
            "Accuracy:42.5%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 6 ------\n",
            "Loss:27.062382972393376\n",
            "Accuracy:37.5%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 7 ------\n",
            "Loss:26.27431016603922\n",
            "Accuracy:27.500000000000004%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 8 ------\n",
            "Loss:25.613179869676472\n",
            "Accuracy:21.25%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 9 ------\n",
            "Loss:25.048173264532274\n",
            "Accuracy:13.750000000000002%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 10 ------\n",
            "Loss:24.55872434232672\n",
            "Accuracy:13.750000000000002%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 11 ------\n",
            "Loss:24.129959218383462\n",
            "Accuracy:12.5%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 12 ------\n",
            "Loss:23.750605651656834\n",
            "Accuracy:12.5%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 13 ------\n",
            "Loss:23.41186849525762\n",
            "Accuracy:12.5%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 14 ------\n",
            "Loss:23.1067443557156\n",
            "Accuracy:11.25%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 15 ------\n",
            "Loss:22.829566461701752\n",
            "Accuracy:11.25%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 16 ------\n",
            "Loss:22.575684816342168\n",
            "Accuracy:12.5%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 17 ------\n",
            "Loss:22.34123266540081\n",
            "Accuracy:13.750000000000002%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 18 ------\n",
            "Loss:22.122951190291698\n",
            "Accuracy:17.5%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 19 ------\n",
            "Loss:21.91805494175899\n",
            "Accuracy:20.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 20 ------\n",
            "Loss:21.724126453811927\n",
            "Accuracy:28.749999999999996%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 21 ------\n",
            "Loss:21.539032040088642\n",
            "Accuracy:32.5%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 22 ------\n",
            "Loss:21.360853038496746\n",
            "Accuracy:36.25%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 23 ------\n",
            "Loss:21.187828266070273\n",
            "Accuracy:41.25%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 24 ------\n",
            "Loss:21.01830446247991\n",
            "Accuracy:47.5%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 25 ------\n",
            "Loss:20.850692205578753\n",
            "Accuracy:48.75%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 26 ------\n",
            "Loss:20.683425281560538\n",
            "Accuracy:50.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 27 ------\n",
            "Loss:20.514921859087096\n",
            "Accuracy:50.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 28 ------\n",
            "Loss:20.343546109509973\n",
            "Accuracy:50.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 29 ------\n",
            "Loss:20.16756919006228\n",
            "Accuracy:50.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 30 ------\n",
            "Loss:19.98512882847411\n",
            "Accuracy:50.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 31 ------\n",
            "Loss:19.79418720175037\n",
            "Accuracy:50.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 32 ------\n",
            "Loss:19.59248750997084\n",
            "Accuracy:50.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 33 ------\n",
            "Loss:19.37751077987981\n",
            "Accuracy:50.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 34 ------\n",
            "Loss:19.146436228090202\n",
            "Accuracy:50.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 35 ------\n",
            "Loss:18.896111265946317\n",
            "Accuracy:58.75%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 36 ------\n",
            "Loss:18.623041248284174\n",
            "Accuracy:77.5%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 37 ------\n",
            "Loss:18.32341454015727\n",
            "Accuracy:93.75%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 38 ------\n",
            "Loss:17.993185132989932\n",
            "Accuracy:98.75%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 39 ------\n",
            "Loss:17.628241569632664\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 40 ------\n",
            "Loss:17.22469408642967\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 41 ------\n",
            "Loss:16.779305617561075\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 42 ------\n",
            "Loss:16.290068088603185\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 43 ------\n",
            "Loss:15.756875649326695\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 44 ------\n",
            "Loss:15.182173124826793\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 45 ------\n",
            "Loss:14.571384415869618\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 46 ------\n",
            "Loss:13.932900781736251\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 47 ------\n",
            "Loss:13.2774858028225\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 48 ------\n",
            "Loss:12.617139524538796\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 49 ------\n",
            "Loss:11.963681797351711\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 50 ------\n",
            "Loss:11.32743559239164\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 51 ------\n",
            "Loss:10.716335897724404\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 52 ------\n",
            "Loss:10.135599946762937\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 53 ------\n",
            "Loss:9.587891728114263\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 54 ------\n",
            "Loss:9.073796884492472\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 55 ------\n",
            "Loss:8.592411904684809\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 56 ------\n",
            "Loss:8.141903941227783\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 57 ------\n",
            "Loss:7.719965575523084\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 58 ------\n",
            "Loss:7.324142962538305\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 59 ------\n",
            "Loss:6.952047886301006\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 60 ------\n",
            "Loss:6.601477964765969\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 61 ------\n",
            "Loss:6.270471409911819\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 62 ------\n",
            "Loss:5.957319245627055\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 63 ------\n",
            "Loss:5.660552504750732\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 64 ------\n",
            "Loss:5.378916667247447\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 65 ------\n",
            "Loss:5.111341286525993\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 66 ------\n",
            "Loss:4.856909555864494\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 67 ------\n",
            "Loss:4.6148303712641745\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 68 ------\n",
            "Loss:4.384414032631108\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 69 ------\n",
            "Loss:4.165051872770755\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 70 ------\n",
            "Loss:3.9561996311070002\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 71 ------\n",
            "Loss:3.75736415949877\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 72 ------\n",
            "Loss:3.5680929649804796\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 73 ------\n",
            "Loss:3.3879660947176546\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 74 ------\n",
            "Loss:3.2165899112764444\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 75 ------\n",
            "Loss:3.0535923668218166\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 76 ------\n",
            "Loss:2.898619449003811\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 77 ------\n",
            "Loss:2.7513325316532438\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 78 ------\n",
            "Loss:2.611406416617603\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 79 ------\n",
            "Loss:2.4785278980837795\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 80 ------\n",
            "Loss:2.3523947177680746\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 81 ------\n",
            "Loss:2.2327148092178453\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 82 ------\n",
            "Loss:2.119205753197133\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 83 ------\n",
            "Loss:2.0115943847705724\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 84 ------\n",
            "Loss:1.909616507218666\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 85 ------\n",
            "Loss:1.8130166791469662\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 86 ------\n",
            "Loss:1.7215480497879263\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 87 ------\n",
            "Loss:1.6349722241041413\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 88 ------\n",
            "Loss:1.5530591443388437\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 89 ------\n",
            "Loss:1.4755869784819706\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 90 ------\n",
            "Loss:1.4023420090080774\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 91 ------\n",
            "Loss:1.333118517413343\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 92 ------\n",
            "Loss:1.2677186617011018\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 93 ------\n",
            "Loss:1.2059523451681793\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 94 ------\n",
            "Loss:1.1476370757272258\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 95 ------\n",
            "Loss:1.0925978156394747\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 96 ------\n",
            "Loss:1.0406668219863477\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 97 ------\n",
            "Loss:0.9916834785217958\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 98 ------\n",
            "Loss:0.9454941197545226\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 99 ------\n",
            "Loss:0.9019518482365307\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 100 ------\n",
            "Loss:0.8609163461019602\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 101 ------\n",
            "Loss:0.8222536819233384\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 102 ------\n",
            "Loss:0.7858361139430934\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 103 ------\n",
            "Loss:0.7515418907055005\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 104 ------\n",
            "Loss:0.719255050065239\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 105 ------\n",
            "Loss:0.6888652174886276\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 106 ------\n",
            "Loss:0.6602674044966341\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 107 ------\n",
            "Loss:0.6333618080278951\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 108 ------\n",
            "Loss:0.6080536114276235\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 109 ------\n",
            "Loss:0.5842527876962084\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 110 ------\n",
            "Loss:0.5618739055607893\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 111 ------\n",
            "Loss:0.5408359388650446\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 112 ------\n",
            "Loss:0.5210620797075853\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 113 ------\n",
            "Loss:0.5024795556980728\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 114 ------\n",
            "Loss:0.4850194516428027\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 115 ------\n",
            "Loss:0.4686165359181679\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 116 ------\n",
            "Loss:0.45320909174111124\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 117 ------\n",
            "Loss:0.43873875350048963\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 118 ------\n",
            "Loss:0.42515034827200154\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 119 ------\n",
            "Loss:0.4123917426018873\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 120 ------\n",
            "Loss:0.40041369461088017\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 121 ------\n",
            "Loss:0.38916971143960793\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 122 ------\n",
            "Loss:0.37861591202968253\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 123 ------\n",
            "Loss:0.3687108952108449\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 124 ------\n",
            "Loss:0.3594156130435362\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 125 ------\n",
            "Loss:0.35069324934797946\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 126 ------\n",
            "Loss:0.3425091033350165\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 127 ------\n",
            "Loss:0.3348304782404095\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 128 ------\n",
            "Loss:0.32762657485286734\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 129 ------\n",
            "Loss:0.3208683898165062\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 130 ------\n",
            "Loss:0.31452861858065784\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 131 ------\n",
            "Loss:0.30858156286368993\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 132 ------\n",
            "Loss:0.3030030424926955\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 133 ------\n",
            "Loss:0.297770311477347\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 134 ------\n",
            "Loss:0.29286197817380055\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 135 ------\n",
            "Loss:0.2882579293931349\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 136 ------\n",
            "Loss:0.28393925830829414\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 137 ------\n",
            "Loss:0.27988819601377696\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 138 ------\n",
            "Loss:0.27608804659328007\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 139 ------\n",
            "Loss:0.27252312555205904\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 140 ------\n",
            "Loss:0.2691787014728393\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 141 ------\n",
            "Loss:0.26604094075662754\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 142 ------\n",
            "Loss:0.26309685531264065\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 143 ------\n",
            "Loss:0.260334253064771\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 144 ------\n",
            "Loss:0.257741691145431\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 145 ------\n",
            "Loss:0.2553084316512797\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 146 ------\n",
            "Loss:0.2530243998391164\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 147 ------\n",
            "Loss:0.2508801446441532\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 148 ------\n",
            "Loss:0.24886680140687334\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 149 ------\n",
            "Loss:0.24697605669873102\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 150 ------\n",
            "Loss:0.2452001151410223\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 151 ------\n",
            "Loss:0.24353166811531934\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 152 ------\n",
            "Loss:0.24196386426792338\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 153 ------\n",
            "Loss:0.24049028171477704\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 154 ------\n",
            "Loss:0.23910490185725816\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 155 ------\n",
            "Loss:0.23780208472312397\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 156 ------\n",
            "Loss:0.23657654575071246\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 157 ------\n",
            "Loss:0.2354233339381927\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 158 ------\n",
            "Loss:0.2343378112832883\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 159 ------\n",
            "Loss:0.23331563344240483\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 160 ------\n",
            "Loss:0.2323527315414967\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 161 ------\n",
            "Loss:0.2314452950743094\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 162 ------\n",
            "Loss:0.2305897558268066\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 163 ------\n",
            "Loss:0.2297827727696643\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 164 ------\n",
            "Loss:0.22902121786366206\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 165 ------\n",
            "Loss:0.22830216272564247\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 166 ------\n",
            "Loss:0.22762286610541843\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 167 ------\n",
            "Loss:0.2269807621266386\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 168 ------\n",
            "Loss:0.22637344924709163\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 169 ------\n",
            "Loss:0.22579867989633579\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 170 ------\n",
            "Loss:0.2252543507508155\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 171 ------\n",
            "Loss:0.2247384936087925\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 172 ------\n",
            "Loss:0.2242492668295072\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 173 ------\n",
            "Loss:0.22378494730294662\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 174 ------\n",
            "Loss:0.22334392291848099\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 175 ------\n",
            "Loss:0.22292468550241618\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 176 ------\n",
            "Loss:0.22252582419620606\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 177 ------\n",
            "Loss:0.2221460192486757\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 178 ------\n",
            "Loss:0.2217840361971375\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 179 ------\n",
            "Loss:0.2214387204137366\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 180 ------\n",
            "Loss:0.22110899199472162\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 181 ------\n",
            "Loss:0.22079384097164714\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 182 ------\n",
            "Loss:0.22049232282474268\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 183 ------\n",
            "Loss:0.22020355427984226\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 184 ------\n",
            "Loss:0.2199267093713721\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 185 ------\n",
            "Loss:0.21966101575493333\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 186 ------\n",
            "Loss:0.21940575125399847\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 187 ------\n",
            "Loss:0.21916024062616354\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 188 ------\n",
            "Loss:0.21892385253527955\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 189 ------\n",
            "Loss:0.21869599671660303\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 190 ------\n",
            "Loss:0.21847612132289598\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 191 ------\n",
            "Loss:0.2182637104401214\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 192 ------\n",
            "Loss:0.21805828176209113\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 193 ------\n",
            "Loss:0.21785938441405736\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 194 ------\n",
            "Loss:0.21766659691586346\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 195 ------\n",
            "Loss:0.21747952527583855\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 196 ------\n",
            "Loss:0.2172978012071697\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 197 ------\n",
            "Loss:0.2171210804589928\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 198 ------\n",
            "Loss:0.21694904125492187\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------Iterative 199 ------\n",
            "Loss:0.21678138283219936\n",
            "Accuracy:100.0%\n",
            "------------Iterative Done------ \n",
            "\n",
            "------------ Training Complete ------ \n",
            "\n",
            "------------ TEST Result ------\n",
            "Loss:0.07708305810292525\n",
            "Accuracy:100.0%\n",
            "------------ TEST Result ------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}